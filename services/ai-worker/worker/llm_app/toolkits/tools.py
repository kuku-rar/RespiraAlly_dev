import json
import os
from typing import Dict, List, Type

from crewai.tools import BaseTool
from openai import OpenAI
from pydantic import BaseModel, Field
from pymilvus import Collection, connections

from ..embedding import to_vector
from .redis_store import commit_summary_chunk

_milvus_loaded = False
_collection = None


class MemoryGateToolSchema(BaseModel):
    text: str = Field(..., description="ä½¿ç”¨è€…æœ¬è¼ªè¼¸å…¥")


class MemoryGateTool(BaseTool):
    name: str = "memory_gate"
    description: str = "åˆ¤æ–·æ˜¯å¦éœ€è¦æª¢ç´¢å€‹äººé•·æœŸè¨˜æ†¶ã€‚åªè¼¸å‡º USE æˆ– SKIPã€‚"
    args_schema: Type[BaseModel] = MemoryGateToolSchema

    def _run(self, text: str) -> str:
        try:
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            sys = (
                "ä½ æ˜¯æ±ºç­–å™¨ã€‚è‹¥è¼¸å…¥æ¶‰åŠå€‹äººæ—¢å¾€äº‹å¯¦/åå¥½/é™åˆ¶/ç”¨è—¥/é†«å›‘/æ’ç¨‹/å®¶äººç¨±å‘¼/ä¸Šä¸€è¼ªå…§å®¹çš„æŒ‡æ¶‰ï¼Œ"
                "æˆ–å‡ºç¾ã€ä¸Šæ¬¡/ä¹‹å‰/ä¸€æ¨£/é‚£å€‹/é‚„æ˜¯/ä¸è¦/éæ•/é†«å¸«èªª/å›ºå®š/æé†’ã€ç­‰å­—çœ¼ï¼Œå› USEï¼›"
                "å¦å‰‡å› SKIPã€‚åªè¼¸å‡º USE æˆ– SKIPã€‚"
            )
            res = client.chat.completions.create(
                model=os.getenv("GUARD_MODEL", os.getenv("MODEL_NAME", "gpt-4o-mini")),
                temperature=0,
                max_tokens=4,
                messages=[
                    {"role": "system", "content": sys},
                    {"role": "user", "content": text},
                ],
            )
            out = (res.choices[0].message.content or "").strip().upper()
            return "USE" if out.startswith("USE") else "SKIP"
        except Exception:
            # å¤±æ•—æ™‚ä¿å®ˆï¼šç›´æ¥ SKIPï¼Œé¿å…å¡æµç¨‹
            return "SKIP"


_milvus_loaded = False
_collection = None


def _index_type_of(col: Collection) -> str:
    idx = (col.indexes or [None])[0]
    it = (idx and idx.params.get("index_type")) or ""
    if not it and idx and isinstance(idx.params.get("params"), str):
        try:
            it = json.loads(idx.params["params"]).get("index_type", "")
        except Exception:
            pass
    return (it or "HNSW").upper()


def _search_param(idx_type: str) -> Dict:
    if idx_type.startswith("IVF"):
        return {
            "metric_type": "COSINE",
            "params": {"nprobe": int(os.getenv("COPD_NPROBE", 32))},
        }
    return {"metric_type": "COSINE", "params": {"ef": int(os.getenv("COPD_EF", 128))}}


class SearchMilvusToolSchema(BaseModel):
    query: str = Field(
        ..., description="ç•¶å‰è¦æŸ¥è©¢çš„è‡ªç„¶èªå¥ï¼ˆä½¿ç”¨è€…æå•æˆ–ä½ è½‰è¿°çš„é—œéµå¥ï¼‰"
    )
    topk: int = Field(5, description="æœ€å¤šæ“·å–çš„å€™é¸æ•¸é‡ï¼ˆé è¨­5ï¼‰")


class SearchMilvusTool(BaseTool):
    name: str = "search_milvus"
    description: str = (
        "æª¢ç´¢ COPD æ•™è‚²èˆ‡è¡›æ•™å•ç­”è³‡æ–™åº«ã€‚ç•¶ä½ éœ€è¦å®¢è§€çŸ¥è­˜ï¼ˆå¦‚ç–¾ç—…æ¦‚å¿µã€ç—‡ç‹€ã€é¢¨éšªã€å°±é†«æ™‚æ©Ÿã€"
        "ç”Ÿæ´»è¡›æ•™ã€è‡ªæˆ‘ç…§è­·ç­‰ï¼‰æˆ–ä½ å°ç­”æ¡ˆä¾†æºä¸ç¢ºå®šæ™‚ï¼Œå…ˆå‘¼å«æœ¬å·¥å…·ã€‚å·¥å…·æœƒå›å‚³ä¸€æ®µå¯ç›´æ¥æ‹¼å…¥"
        "æç¤ºè©çš„ã€åƒè€ƒè³‡æ–™ã€å€å¡Šï¼Œå…§å«ç›¸ä¼¼åº¦æœ€é«˜çš„ä¸€ç­† Q&A èˆ‡ä½¿ç”¨èªªæ˜ï¼Œä¾›ä½ ç†è§£ä¸¦è½‰è¿°æ•´åˆã€‚"
    )
    args_schema: Type[BaseModel] = SearchMilvusToolSchema

    def _run(self, query: str, topk: int = 5) -> str:
        global _milvus_loaded, _collection
        try:
            if not _milvus_loaded:
                try:
                    connections.get_connection("default")
                except Exception:
                    connections.connect(
                        alias="default",
                        uri=os.getenv("MILVUS_URI", "http://localhost:19530"),
                    )
                coll_name = os.getenv("COPD_COLL_NAME", "copd_qa")
                _collection = Collection(coll_name)
                _collection.load()
                _milvus_loaded = True

            vec = to_vector(query)
            if not vec:
                return json.dumps({"source": "copd_qa", "hits": []}, ensure_ascii=False)
            if not isinstance(vec, list):
                vec = vec.tolist() if hasattr(vec, "tolist") else list(vec)

            idx_type = _index_type_of(_collection)
            param = _search_param(idx_type)
            res = _collection.search(
                data=[vec],
                anns_field="embedding",
                param=param,
                limit=topk,
                output_fields=["question", "answer", "category", "keywords", "notes"],
            )

            thr = float(os.getenv("SIMILARITY_THRESHOLD", 0.7))
            hits: List[dict] = []
            for h in res[0]:
                score = float(getattr(h, "distance", getattr(h, "score", 0.0)))
                if score >= thr:
                    e = h.entity
                    hits.append(
                        {
                            "score": score,
                            "q": e.get("question", ""),
                            "a": e.get("answer", ""),
                            "cat": e.get("category", ""),
                            "kw": e.get("keywords", ""),
                            "notes": e.get("notes", ""),
                        }
                    )
            if not hits:
                return (
                    "ğŸ“š åƒè€ƒè³‡æ–™ï¼šæœªæ‰¾åˆ°ç›¸ç¬¦æ¢ç›®ï¼ˆå¯èƒ½ç„¡è³‡æ–™æˆ–ç›¸ä¼¼åº¦ä¸è¶³ï¼‰ã€‚"
                    "è‹¥ä½ ä»éœ€å›ç­”ï¼Œè«‹åŸºæ–¼é€šç”¨å¸¸è­˜èˆ‡ç›®å‰å°è©±è„ˆçµ¡ï¼Œç°¡æ½”å›è¦†ï¼›é¿å…æœæ’°ã€‚"
                )

            # å–ç›¸ä¼¼åº¦æœ€é«˜ä¸€ç­†
            best = max(hits, key=lambda x: x.get("score", 0.0))
            score_txt = f"{best.get('score', 0.0):.3f}"
            q = (best.get("q") or "").strip()
            a = (best.get("a") or "").strip()

            # å›å‚³å¯ç›´æ¥åµŒå…¥ LLM æç¤ºçš„å€å¡Šï¼Œä¸¦é™„ä¸Šä½¿ç”¨èªªæ˜
            return (
                "ğŸ“š åƒè€ƒè³‡æ–™ï¼ˆMilvus COPD QAï¼Œå·²æŒ‘ç›¸ä¼¼åº¦æœ€é«˜ä¸€ç­†ï¼›metric=COSINEï¼›score="
                + score_txt
                + ")ï¼š\n"
                + ("Q: " + q + "\n" if q else "")
                + ("A: " + a + "\n" if a else "")
                + "ä½¿ç”¨æ–¹å¼ï¼šå°‡ A çš„é‡é»è½‰è¿°ç‚ºè‡ªç„¶å£èªä¸¦çµåˆç•¶å‰è„ˆçµ¡ï¼›è‹¥ä¸ç›¸ç¬¦æˆ–éæ™‚ï¼Œè«‹å¿½ç•¥ã€‚ä¸è¦é€å­—è²¼ä¸Šæˆ–å¤–æ´©æ•æ„Ÿè³‡è¨Šã€‚"
            )
        except Exception as e:
            return f"ğŸ“š åƒè€ƒè³‡æ–™ï¼šæª¢ç´¢å¤±æ•—ï¼ˆ{str(e)}ï¼‰ã€‚è‹¥ä½ ä»éœ€å›ç­”ï¼Œè«‹åŸºæ–¼é€šç”¨å¸¸è­˜èˆ‡ç›®å‰è„ˆçµ¡ï¼Œç°¡æ½”å›è¦†ï¼›é¿å…æœæ’°ã€‚"


def summarize_chunk_and_commit(
    user_id: str, start_round: int, history_chunk: list
) -> bool:
    if not history_chunk:
        return True
    text = "".join(
        [
            f"ç¬¬{start_round + i + 1}è¼ª:\né•·è¼©: {h['input']}\né‡‘å­«: {h['output']}\n\n"
            for i, h in enumerate(history_chunk)
        ]
    )
    prompt = f"è«‹å°‡ä¸‹åˆ—å°è©±åš 80-120 å­—æ‘˜è¦ï¼Œèšç„¦ï¼šå¥åº·å•é¡Œã€æƒ…ç·’ã€ç”Ÿæ´»è¦é»ã€‚\n\n{text}"
    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        res = client.chat.completions.create(
            model=os.getenv("MODEL_NAME", "gpt-4o-mini"),
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­çš„å°è©±æ‘˜è¦åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
        )
        body = (res.choices[0].message.content or "").strip()
        header = f"--- ç¬¬{start_round + 1}è‡³{start_round + len(history_chunk)}è¼ªå°è©±æ‘˜è¦ ---\n"
        return commit_summary_chunk(
            user_id,
            expected_cursor=start_round,
            advance=len(history_chunk),
            add_text=header + body,
        )
    except Exception as e:
        print(f"[æ‘˜è¦éŒ¯èª¤] {e}")
        return False


class AlertCaseManagerToolSchema(BaseModel):
    reason: str = Field(
        ...,
        description="ä¸€è¡ŒåŸå› å­—ä¸²ï¼Œéœ€ä»¥ 'EMERGENCY:' é–‹é ­ï¼Œä¾‹å¦‚ï¼š'EMERGENCY: suicidal ideation'",
    )


class AlertCaseManagerTool(BaseTool):
    name: str = "alert_case_manager"
    description: str = (
        "åµæ¸¬åˆ°ç·Šæ€¥å¥åº·/å¿ƒç†é¢¨éšªæ™‚ï¼Œç«‹å³é€šå ±å€‹ç®¡å¸«ã€‚"
        "ã€ç”¨æ³•ã€‘ä»¥ JSON å‚³å…¥ {'reason': 'EMERGENCY: <æ¥µç°¡åŸå› >'}ï¼›"
        "ç”¨æˆ¶IDç”±ç³»çµ±è‡ªå‹•å¡«å…¥ï¼Œç„¡éœ€æä¾›ã€‚"
    )
    args_schema: Type[BaseModel] = AlertCaseManagerToolSchema  # â˜… é—œéµï¼šæ˜ç¢ºå®£å‘Šåƒæ•¸éµ

    def _run(self, reason: str) -> str:
        # å®‰å…¨åœ°æŠ“ user_idï¼›CrewAI é è¨­æ²’æœ‰ runtime_context
        uid = None
        try:
            uid = (getattr(self, "runtime_context", {}) or {}).get("user_id")
        except Exception:
            pass
        uid = uid or os.getenv("CURRENT_USER_ID") or "unknown"

        from datetime import datetime

        ts = datetime.now().isoformat(timespec="seconds")
        print(f"[{ts}] ğŸš¨ AlertCaseManagerTool triggered: user={uid}, reason={reason}")
        # é€™è£¡æœ¬ä¾†æœ‰ MQ ç™¼é€çš„è¨»è§£ç¢¼ï¼Œä¿ç•™å³å¯
        from .rabbitmq_publisher import publish_alert
        publish_alert(user_id=uid, reason=reason)
        return f"âš ï¸ å·²é€šå ±å€‹ç®¡å¸«ä½¿ç”¨è€…ID: {uid}ï¼Œäº‹ç”±ï¼š{reason}"


class ModelGuardrailTool(BaseTool):
    name: str = "model_guardrail"
    description: str = (
        "å®Œå…¨ç”± LLM å®‰å…¨å¯©æŸ¥ï¼šåˆ¤æ–·æ˜¯å¦é•æ³•/å±éšª/è‡ªå‚·ï¼Œæˆ–å±¬æ–¼éœ€å°ˆæ¥­äººå£«çš„å…·é«”æŒ‡ç¤ºï¼ˆå¦‚ç”¨è—¥/åŠ‘é‡/è¨ºæ–·/è™•ç½®ï¼‰ã€‚åªè¼¸å‡º OK æˆ– BLOCK: <åŸå› >ã€‚"
    )

    def _run(self, text: str) -> str:
        try:
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            guard_model = os.getenv(
                "GUARD_MODEL", os.getenv("MODEL_NAME", "gpt-4o-mini")
            )
            sys = (
                "ä½ æ˜¯å®‰å…¨å¯©æŸ¥å™¨ï¼Œéœ€ç²¾æº–åˆ¤æ–·æ˜¯å¦å¿…é ˆæ””æˆªã€‚\n"
                "ã€å…è¨±ï¼ˆå› OKï¼‰ã€‘ä»¥ä¸‹æƒ…æ³ä¸€å¾‹æ”¾è¡Œï¼š\n"
                "- ç—‡ç‹€æ•˜è¿°æˆ–ä¸€èˆ¬å¥åº·è³‡è¨Šçš„è©¢å•ï¼ˆä¾‹å¦‚ï¼šç—‡ç‹€ã€æˆå› ã€é¢¨éšªå› å­ã€ä½•æ™‚å°±é†«çš„æé†’ï¼‰ã€‚\n"
                "- ä¸€èˆ¬ç”Ÿæ´»ä¿å¥/è¡›æ•™/æ³¨æ„äº‹é …ç­‰éå€‹æ¡ˆåŒ–ã€éæŒ‡ä»¤æ€§çš„å»ºè­°ã€‚\n"
                "- æƒ…ç·’ä½è½ã€è‡ªæ®ºå¿µé ­æˆ–æ±‚åŠ©çš„è¡¨é”ï¼ˆä¸å«å…·é«”æ–¹æ³•ï¼‰ã€‚\n"
                "ã€å¿…é ˆæ””æˆªï¼ˆå› BLOCK: <åŸå› >ï¼‰ã€‘ç¬¦åˆä»»ä¸€æ¢ä»¶ï¼š\n"
                "1) é•æ³•/å±éšªè¡Œç‚ºçš„æ•™å­¸ã€è²·è³£ã€è£½ä½œæˆ–è¦é¿ï¼ˆæ¯’å“ã€æ­¦å™¨ã€æš´åŠ›ã€é§­å®¢ã€çˆ†è£‚ç‰©ç­‰ï¼‰ã€‚\n"
                "2) è‡ªå‚·/ä»–å‚·/è‡ªæ®º/è‡ªæ®˜çš„ã€å…·é«”æ–¹æ³•æŒ‡å°æˆ–é¼“å‹µåŸ·è¡Œã€ã€‚\n"
                "3) æˆäººæ€§å…§å®¹æˆ–æœªæˆå¹´ç›¸é—œä¸ç•¶å…§å®¹çš„è«‹æ±‚ã€‚\n"
                "4) é†«ç™‚/ç”¨è—¥/åŠ‘é‡/è¨ºæ–·/è™•ç½®ç­‰ã€å…·é«”ã€å€‹æ¡ˆåŒ–ã€å¯åŸ·è¡Œã€çš„å°ˆæ¥­æŒ‡ç¤ºæˆ–æ–¹æ¡ˆã€‚\n"
                "5) æ³•å¾‹ã€æŠ•è³‡ã€ç¨…å‹™ç­‰é«˜é¢¨éšªé ˜åŸŸä¹‹ã€å…·é«”ã€å¯åŸ·è¡Œã€çš„å°ˆæ¥­æŒ‡å°ã€‚\n"
                "ã€åˆ¤æ–·åŸå‰‡ã€‘åƒ…åœ¨è«‹æ±‚æ˜ç¢ºè½å…¥ä¸Šè¿°æ””æˆªæ¢ä»¶æ™‚æ‰ BLOCKï¼›\n"
                "è‹¥æ˜¯æè¿°ç‹€æ³æˆ–å°‹æ±‚ä¸€èˆ¬æ€§èªªæ˜/ä¿å¥å»ºè­°ï¼Œè«‹å› OKã€‚\n"
                "è‹¥ä¸ç¢ºå®šï¼Œé è¨­å› OKã€‚\n"
                "ã€è¼¸å‡ºæ ¼å¼ã€‘åªèƒ½æ˜¯ï¼š\n"
                "OK\n"
                "æˆ–\n"
                "BLOCK: <æ¥µç°¡åŸå› >\n"
            )

            user = f"ä½¿ç”¨è€…è¼¸å…¥ï¼š{text}\nè«‹ä¾è¦å‰‡åªè¼¸å‡º OK æˆ– BLOCK: <åŸå› >ã€‚"
            res = client.chat.completions.create(
                model=guard_model,
                messages=[
                    {"role": "system", "content": sys},
                    {"role": "user", "content": user},
                ],
                temperature=0,
                max_tokens=24,
            )
            out = (res.choices[0].message.content or "").strip()
            # é è¨­å¯¬é¬†é€šéï¼šè‹¥éæ˜ç¢º BLOCKï¼Œä¸€å¾‹è¦–ç‚º OK
            if not out.startswith("BLOCK:"):
                return "OK"
            # åƒ…ä¿ç•™ç²¾ç°¡ BLOCK ç†ç”±
            if len(out) > 256:
                out = out[:256]
            return out
        except Exception as e:
            # Guardrail æ•…éšœæ™‚ï¼Œä¸è¦é˜»æ“‹ä¸»æµç¨‹
            print(f"[guardrail_error] {e}")
            return "OK"
